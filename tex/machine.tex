\documentclass[../main.tex]{subfiles}
\begin{document}
	\chapter{Machine Learning} \label{ch:machine}
	

	\section{Machine Learning Basics}
	\noindent 
	
	\noindent  \textit{Machine Learning} focuses on the development of algorithms and models that enable computers to learn from data with the aim of making predictions without being explicitly programmed.\\ \\  
	We can think about learning as the way we understand it as a human. We can classify a learning problem based on the degree of feedback. Machine learning models fall into three primary categories:
	\begin{itemize}
		\item Supervised learning, where we have immidiate feedback.
		\item Reinforcement learning, where we have indirect feedback. For example whenwe are playing the game of chess.
		\item Unsupervised learning, where we have non feedback signal. For example,deducing which dog belongs to each owner.
	\end{itemize}
	Machine learning models simplify reality for the purposes of understanding or prediction. This prediction can be either a numerical prediction or a classifications prediction. 
	A number of machine learning algorithms are commonly used.
	
	
	\subsection{Motivation }
	\noindent Consider the problem of assessing the eligibility of a consumer for a credit. We are provided with the following set of data: \\ 
	
	\begin{tabular}{ll}
		\toprule
		\textbf{Costumer application:}  \\ 
		\midrule
		Age & 23 years \\
		Gender & Male \\
		Annual Salary & \$30,000 \\
		Years in Residence & 1 year \\
		Years in Job & 1 year \\
		Current Debt & \$15,000 \\
		... & ... \\
		\bottomrule
	\end{tabular} 
	\\ \\
	\begin{itemize}
		\item Input: \textbf{$x_c=(x_{c_1},...,x_{c_d})$}  "attributes of the costumer that we want to classify". 
		\item Output:  
		\[
		y = \begin{cases}
			approve \\
			deny& \\
		\end{cases}
		\]
		
		\item Target funtion: $f$  "ideal credit approval formula"
		\item Data: The set $\{(x_1, y_1),(x_2,y_2),...,(x_n ,y_n)\}$ corresponds of a historical records of credit customers where $x_i$ is the attributes of the costumer and $y_i$ classification awarded.
	\end{itemize}
	
	\noindent We are looking for the function $f$ such that $f(x_c)=y$.
	\\ \\ 
	
	\noindent 
	A fundamental problem of machine learning is the following. Given data of the form $\{(x_i,y_i)\}^m_{i=1} \subset \mathbb{R}^n \times \mathbb{R}$, drawn randomly from a probability distribution $\mu$, find a model P such that $P(x_i)=y_i$. An important aspect of machine learning is that \textit{many supervised learning tasks are about function learning.}. 
	\begin{xmpl}
		\noindent An example of a supervised learning task is digit recognition. The objective is to identify handwritten digits (0-9) based on input images. In this task, we aim to learn a probability distribution function denoted as $f$, which maps a set of pixel values ranging from 0 (black) to 255 (white), representing a 28x28 image, to a probability distribution over the digits 0 to 9.
		
		$$f: \{0,..., 255\}^{28 \times 28} \longrightarrow \text{probability distribution on } \{0,1,...,9\}$$
		
	\end{xmpl}
	\begin{xmpl} Example of a classification problem. We want to classify if an image is a dog or not a dog. We would like to produce a value which is correlated with the probability of this image being a dog or not a dog.  We can approach the problem in the following way. We want to find a function that takes very high values when dog-image and very low val ues when non dog images and takes the value 0 when its uncertain. 
		$$d: \mathbb{R}^{\# \text{pixels in image}} \rightarrow \mathbb{R} $$
		such that $\mathbb{P}(d(\text{image})) = \text{probability that the image is a dog.}$ \\ \\  
		That is what we mean by many problems can be recast as function learning. Note that there is not a god-given reason why this function should exist. We know that certain points in space, and they have certain values associated to them, but we dont know that there is some big function. 
	\end{xmpl}
	
	\noindent \textit{Important principle $II$}: Sometimes function learning can be recast as a classification problem.  
	\\ \\ 
	Binary classification problem. 
	Rather learning $\mu : \mathbb{R}^{\# \text{bits}} \rightarrow \mathbb{R}$ where big values correspond to likely and small values to unlikely. It is better to learn $\mu : \mathbb{R}^{\# \text{bits}} \rightarrow $ probability distribution on $\{-1,0,1\}$. In number theory the function $\mu(n)$ it is called Möebius function 
	
	\[
	\mu(n) = \begin{cases}
		0 & \text{if } n \text{ has a repeated square factor}, \\
		-1 & \text{if } n \text{ has an odd number of distinct prime factors}, \\
		1 & \text{if } n \text{ has an even number of distinct prime factors}.
	\end{cases}
	\]
	
	\subsection{Linear Regression}
	\noindent A linear regression algorithm is used to predict numerical values, based on a linear relationship between different values. A simple linear model has an outcome, denoted by $y$, also known as a response variable,  and a predictor, $x$. It is defined by the following equation.
	
	$$ y_i= \beta_0 + \beta_1 x_i + \epsilon $$

	 \noindent where $ i = 1, ..., n$  indexes the observations from 1 to $n$ in the dataset.

	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\linewidth]{imgs/gdp.png}
		 \caption{\small $y$ - response variable: unemployment rate , $x$ predictor: GDP growth , } 
	\end{figure} \mbox{} \par

\noindent We can add additional $p$ predictors to a simple linear model, transforming it into a multivariate linear model, which we define as follows:

$$y_i = \beta_0 + \beta_1x_{1i} + \ldots + \beta_px_{pi} + \epsilon_i $$

\subsection{Logistic Regression}
\noindent Logistic regression is a model for predicting the probability that a binary response is 1.  It is suitable for classification tasks, as well as for prediction of probabilities.  From a statistical perspective, it is defined by assuming that the distribution of the binary response variable, $y$, given the features, $x$, follows a Bernoulli
distribution with success probability $p$. 

$$P(y = 1 | X = x) = p $$
We need to define the concept of sigmoid function that will be important along the work. A \textit{sigmoid function} is a mathematical function that maps input values to a range between 0 and 1. We have the following sigmoid function, the logit inverse function:

\[
\text{logit}: (0,1) \to \mathbb{R}
\quad \text{and is expressed as:} \quad
\text{logit}(x) = \log\left(\frac{x}{x-1}\right)
\]


\[
\text{logit}^{-1}: \mathbb{R} \to (0,1)
\quad \text{and is expressed as:} \quad
\text{logit}^{-1}(x) = \frac{e^x}{1+e^x}
\]

\noindent The linear predictor, $ w^T x +\theta$, fluctuates between $(-\infty,\infty)$ where $x$ represents all predictors in the model. To address this difference in scale, the outcome variable is transformed using the logit function. The transformed result, $\text{logit}(x)$, is expressed in logarithms of probabilities. 
The logistic regression model assumes a linear (affine) relationship between the feature vector $x_i$ and the log odds of $p$. Namely,
$$\text{logit}(p) = w^T x +\theta$$

The logistic model can be alternatively expressed using the inverse logit function:

\[
P(y = 1 | X=x) = \text{logit}^{-1}(w^T x +\theta)
\]




\section{Multilayer Feedforward Networks}
	
	 
	 \noindent \textit{Artificial Neural Networks} (ANN) are the quintessential deep learning models, especially \textit{multilayer feedforward networks}. They are widely used for nonlinear function approximation. The goal of a artificial neural network is to approximate some function $f^*$. For example, for a classifier, $y = f^*(x)$ maps an input $x$ to a category $y$.  \\ \\ 
	 %A feedforward network defines a mapping $y = f(x;\theta)$ and learns the value of the parameters $\theta$ that result in the best function approximation. \\ \\ 
	 \noindent The term \textit{neural} refers  to the fact that this model was originally inspired by how biological neurons process information. These artificial neurons mimic the processing of information in biological neurons. \\ \\ 
	 \noindent The term \textit{feedforward} indicates the direction of information flow within the network, moving only forward in contraposition to backwards. Each layer processes the input data and passes its output to the next layer, creating a sequence of transformations until the final output is produced. $f(f_1(f_2(f_3)))$ \\ \\
	 \noindent The term \textit{network} refers to the interconnected structure of artificial neurons. An multilayer network consists of multiple layers, including an input layer, one or more hidden layers, and an output layer. \\ \\ %The neurons within each layer are connected to the neurons in the subsequent layer, forming a network of connections. The connections between neurons are represented by weights, which determine the strength and influence of each neuron's output on the inputs of the subsequent layer. \\ \\ 
	 \noindent The architecture of the network entails determining its \textit{depth}, \textit{width}, and \textit{activation functions} used. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer. 
	 
	 
	 %they are composed of interconnected processing units called artificial neurons, which are organized in layers and are capable of learning and generalizing from data.	
	 \section{Architecture of a Multilayer Feedforward Network}
	 \subsection{Artificial neuron}
	 \noindent The equation
	  \begin{equation}
	  	y=\sigma(w^Tx + \theta) \tag{2}
	  	\label{eq:sig}
	  \end{equation}
  
   \noindent represents what we may call a \textit{single layer of a deep learning model}, also called an \textit{artificial neuron}.  Observe that the artificial neuron is composed of an affine transformation $ z=w^Tx + \theta$ followed by a (generally) non-linear transformation $\sigma(z)$. In more detail, $x \in \mathbb{R}^n$ is the input vector and represents a set of $n$ features or predictors, $w\in  \mathbb{R}^n$ is the weights vector. Each element of the weights vector $w_i$ corresponds to the weight or importance assigned to the corresponding input feature $x_i$. $\theta$ is the bias and $\sigma$ is the activation function. The result variable is an scalar output $y \in \mathbb{R}$. 
 
   
   \begin{figure}[h]
   	\centering
   	\includegraphics[width=0.8\linewidth]{imgs/neu}
   \end{figure} \mbox{} \par
   
  
 \subsection{Activation Function}
 \noindent The introduction of the activation function in ANN was inspired by biological neural networks whose purpose is to decide whether a particular neuron fires or not. The simple addition of such a nonlinear function can tremendously help the network to exploit more, thereby learning faster. There are various activation functions proposed in the literature, and it is difficult to find the optimal activation function that can tackle any problem. \\ \\ 
	 	\noindent Note that a logistic regression is an artificial neuron where the activation function  $\sigma$ is $\text{logit}^{-1}$. A few very popular activation function are the Hyperbolic Tangent and the ReLu: 
	

	 \[
	 \text{tanh}: \mathbb{R} \to (-1,1)
	 \]
	 and 
	  \[
	 \text{ReLU}: \mathbb{R} \to (0,\infty)
	 \quad \text{and is expressed as:} \quad
	 \text{ReLU}(x) = \max(0,x)
	 \]
	 
	 \noindent  We now get into more details on the precise definition of a deep neural network, which is after all a purely mathematical object. 
	 \begin{definition} A \textit{multilayer feedforward network} is the function
	 	$$f(x)=\sum_{j=1}^k \beta_j \cdot \sigma(w_j \cdot x - \theta_j)$$
	 	where $x \in \mathbb{R}^n$ is the input vector, $k \in \mathbb{N}$ is the number of processing units in the hidden layer, $w_j \in \mathbb{R}^n$ is the weight vector that connects the input to processing unit $j$ in the hidden layer, $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ is an activation function, $\theta_j \in \mathbb{R}$ is the threshold (or bias) associated with processing unit $j$ in the hidden layer, and $\beta_j \in \mathbb{R}$ is the weight that connects processing unit $j$ in the hidden layer to the output of the network.
	 	
	 	
	 \end{definition}
	 
	 \noindent Let $N_{w}$ be the family of all functions implied by the network's architecture.  If we can show that $N_{w}$ is dense in $C(\mathbb{R}^n)$, we can conclude that for every continuous function $g \in C(\mathbb{R}^n) $ and each compact set $K \subset \mathbb{R}^n$, there is a function $f \in N_{w}$ such that $f$ is a good approximation to $g$ on K. \\ \\
	 \noindent Under which necessary and sufficient conditions on $\sigma$ will the family of networks $N_w$ be capable of approximating to any desired accuracy any given continuous function?
	 
	
\end{document}