\documentclass[../main.tex]{subfiles}
\begin{document}
	\chapter{Machine Learning} \label{ch:machine}
	

	\section{Machine Learning Basics}
	\noindent 
	
	\noindent Machine Learning is the science of programming computers so they can learn from data. The key is that it allows solving a problem without being explicitly programmed. Mitchell (1997) provides a deﬁnition for a machine learning algorithm: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” 
	
	%Machine learning enables us to tackle tasks that are too diﬃcult to solve with ﬁxed programs written and designed by human beings. From a scientiﬁc and philosophical point of view, machine learning is interesting because developing ourunderstanding of it entails developing our understanding of the principles thatunderlie intelligence
	
	
	\subsection{Tasks}
	A machine learning task refers to the problem that the machine learning model aims to solve or accomplish. Machine learning tasks can vary widely depending on the nature of the data and the desired outcome. Some common machine learning tasks include classification, regression, clustering, and anomaly detection.
	
	
	\subsection{Performance Measure}
	\subsection{Experience}
	\section{Types of Learning}
	\noindent  But what do we mean by learning? We can think about learning as the way we understand it as a human. We can classify a learning problem based on the degree of feedback. The three main types are: 
	\begin{enumerate}
		\item Supervised learning, where we have immidiate feedback. %involves learning from labeled data.
		\item Reinforcement learning, where we have indirect feedback. %which involves learning from feedback based on rewards or penalties.
		 For example when we are playing the game of chess.
		\item Unsupervised learning, where we have non feedback signal. %which involves discovering patterns in data without any predefined labels or feedback, 
		For example, deducing which dog belongs to each owner.  
	\end{enumerate}
\subsection{Supervised learning}
	\noindent An example of a supervised learning task is digit recognition. The objective is to identify handwritten digits (0-9) based on input images. In this task, we aim to learn a probability distribution function denoted as $f$, which maps a set of pixel values ranging from 0 (black) to 255 (white), representing a 28x28 image, to a probability distribution over the digits 0 to 9.

$$f: \{0,..., 255\}^{28 \times 28} \longrightarrow \text{probability distribution on } \{0,1,...,9\}$$

	\section{Function Learning}
	\noindent 
	A fundamental problem of machine learning is the following. Given data of the form $\{(x_i,y_i)\}^m_{i=1} \subset \mathbb{R}^n \times \mathbb{R}$, drawn randomly from a probability distribution $\mu$, find a model P such that $P(x_i)=y_i$. An important aspect of machine learning is that \textit{many supervised learning tasks are about function learning.}. 

\begin{xmpl} Example of a classification problem. We want to classify if an image is a dog or not a dog. We would like to produce a value which is correlated with the probability of this image being a dog or not a dog.  We can approach the problem in the following way. We want to find a function that takes very high values when dog-image and very low val ues when non dog images and takes the value 0 when its uncertain. 
	$$d: \mathbb{R}^{\# \text{pixels in image}} \rightarrow \mathbb{R} $$
	such that $\mathbb{P}(d(\text{image})) = \text{probability that the image is a dog.}$ \\ \\  
	That is what we mean by many problems can be recast as function learning. Note that there is not a god-given reason why this function should exist. We know that certain points in space, and they have certain values associated to them, but we dont know that there is some big function. 
\end{xmpl}

\noindent \textit{Important principle $II$}: Sometimes function learning can be recast as a classification problem.  
\\ \\ 
Binary classification problem. 
Rather learning $\mu : \mathbb{R}^{\# \text{bits}} \rightarrow \mathbb{R}$ where big values correspond to likely and small values to unlikely. It is better to learn $\mu : \mathbb{R}^{\# \text{bits}} \rightarrow $ probability distribution on $\{-1,0,1\}$. In number theory the function $\mu(n)$ it is called Möebius function 

\[
\mu(n) = \begin{cases}
	0 & \text{if } n \text{ has a repeated square factor}, \\
	-1 & \text{if } n \text{ has an odd number of distinct prime factors}, \\
	1 & \text{if } n \text{ has an even number of distinct prime factors}.
\end{cases}
\]

	
	
\section{Artificial neural networks }

\noindent \textit{Artificial neural networks} (ANNs) are widely used for nonlinear function approximation.
(nonlinear classifier.)  They were initially inspired by the way biological neurons process information. 

They are composed of interconnected processing units called artificial neurons, which are organized in layers and are capable of learning and generalizing from data.	
	
\end{document}