\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Function Approximation} \label{ch:methods}
\noindent Among the most famous techniques for function approximation, we find interpolation: such as Taylor polynomial, Chebyshev polynomial, the method of least squares, or spline approximation. In this chapter we are going to talk about ...
\section{????}
\noindent In this section we present some mathematical definitions and results of function approximation. 
If we want to approximate functions, we need to define the following notions: distance between functions, density,


\begin{definition}\label{thm:first}
	A \emph{metric} (or \emph{distance}) on a set $X$ is a function $d:X\times X\rightarrow \mathbb{R} $ such that for all $s,t, u\in X$ the following properties are satisfied:
	\begin{enumerate}
		\item $d(s,t) \geq 0$ and $ \, d(s,t)=0$ if and only if $ s=t$.
		\item $d(s,t)=d(t,s)$.
		\item $d(s,t)\leq d(s,u)+d(u,t)\quad$(\emph{triangle inequality}).
	\end{enumerate}
A \emph{metric space} is a pair $(X,d)$, where $X$ is a set and $d$ is a distance in $X$.
\end{definition}
\noindent  If we take $X$ to be a set of functions, the metric $d(f,g)$ will enable us to measure the distance between functions $f,g \in X$.


\begin{definition} We dentoe by $\mathcal{C}(\mathbb{R}^n)$ the set of continuous functions defined on $\mathbb{R}^n$.
\end{definition}

\begin{definition} We denote by
	$ \mathcal{C}^\infty_0$ functions $\mathcal{C}^\infty$ with compact support. Recall that the support of a function $u$ is denoted by $supp(u)= \{x | u(x)\neq 0\}$
\end{definition}
\begin{propo} 
Let $\rho$ be a metric on the set $\mathcal{C}^\infty_0[a,b]$ defined by
$$\rho(\varphi_1,\varphi_2) = \sum_{n=0}^\infty 2^{-n}  \frac{\|\varphi_1 -\varphi_2\|_n}{1+\|\varphi_1 -\varphi_2\|_n}$$ where $\|\varphi\|_n= \sum_{j=0}^n \sup_{x\in [a,b]} | \varphi^{(j)}(x)| $. We can show that $(\mathcal{C}^\infty_0[a,b],\rho)$  is a complete metric space (FrÃ©chet space).
\end{propo}


\noindent Lebesgue measure

\begin{definition} A box in $\mathbb{R}^d$ is a set of the form 
	$$ Q = [a_1,b_1]  \times ... \times [a_d,b_d] = \prod_{i=1}^d [a_i,b_i]$$
	The volume of the box is $$ vol(Q)= (b_1,a_1) ... (b_d-a_d)= \prod_{i=1}^d (b_i-a_i)$$
	The \emph{exterior measure} (or outer measure) of a set $E\subseteq \mathbb{R}^d$ is $$ |E|^* = \inf\{\sum_k vol(Q_k)\} $$ 
	where the infimum is taken over all finite or countable collection of boxes $Q_k$ such that $E \subseteq \cup_k Q_k$
\end{definition}

\begin{definition}
A set $E\subseteq \mathbb{R}^n $ is \emph{Lebesgue mesurable} (or mesurable) if $\forall \epsilon >0$, there exist $U$ open set such that $E \subseteq U$ and $|U\setminus E|^* < \epsilon $
\end{definition}

\begin{definition}
	We say that a property holds almost everywhere (a.e.) if the set of points that doesnâ€™t hold it is null.
\end{definition}
\begin{definition}
	A function $u$ defined almost everywhere on a measurable set $\Omega \in \mathbb{R}^n$ is said to be \emph{essentially bounded} on $\Omega$ if $|u(x)|$ is bounded almost everywhere on $\Omega$. We denote $u\in L^{\infty}(\Omega)$ with the norm $$\|u\|_{L^{\infty}(\Omega)}= inf(\lambda | \{ x : |u(x)| \geq \lambda \} = 0 ) = ess \sup_{x\in \Omega} |u(x)|$$

\end{definition}

\noindent We have that $L^{\infty}(\mathbb{R})$ is the space of essentially bounded functions.
\\ \\ 
Examples and counterexamples of functions essentially bounded. 
\begin{itemize}
\item $f:\Omega \rightarrow $
\end{itemize}

\begin{definition}
	A function u defined almost everywhere on a domain $\Omega$ (a domain is an open set in $\mathbb{R}^n$) is said to be\emph{ locally essentially bounded }on $\Omega$ if for every compact set $K\subset \Omega$, $u\in L^{\infty}(K)$. We denote $u\in L_{loc}^{\infty}(K)$.
\end{definition}

\begin{definition}We say that a set of functions $F\subset L_{loc}^{\infty}(\mathbb{R})$ is \emph{dense} in $C(\mathbb{R}^n)$ if for every function $g\in C(\mathbb{R}^n)$ and for every compact $K\subset \mathbb{R}^n$, there exist a sequence of functions $f_j\in F$ such that $$\lim_{j\rightarrow\infty} \|g-f_j\|_{L^\infty(K)}=0.$$ 
\end{definition}


\begin{definition}
	Let $f,g$ be real-valued functions with compact support. We define the \emph{convolution} of $f$ with $g$ as $$(f\ast g)(x)=\int f(x-t)g(t) \, dt$$
\end{definition}



\section{Multilayer Feedforward Network}
\noindent Multilayer feedforward networks are a type of artificial neural network that consist of several layers of interconnected nodes, with each node taking input from the previous layer and producing output for the next layer. The general architecture of a multilayer feedforward network, MFN, consist of: input layer: n-input units,  one/more hidden layers : intermediate processing units, output layer: m output-units. 

\begin{tikzpicture}[x=2.7cm,y=1.6cm]
	\message{^^JNeural network activation}
	\def\NI{4} % number of nodes in input layers
	\def\NO{2} % number of nodes in output layers
	\def\yshift{0.4} % shift last node for dots
	
	% INPUT LAYER
	\foreach \i [evaluate={\c=int(\i==\NI); \y=\NI/2-\i-\c*\yshift; \index=(\i<\NI?int(\i):"n");}]
	in {1,...,\NI}{ % loop over nodes
		\node[node in,outer sep=0.6] (NI-\i) at (0,\y) {$x_{\index}^{}$};
	}
	
	% OUTPUT LAYER
	\foreach \i [evaluate={\c=int(\i==\NO); \y=\NO/2-\i-\c*\yshift; \index=(\i<\NO?int(\i):"m");}]
	in {\NO,...,1}{ % loop over nodes
		\ifnum\i=1 % high-lighted node
		\node[node hidden]
		(NO-\i) at (1,\y) {$y_{\index}^{}$};
		\foreach \j [evaluate={\index=(\j<\NI?int(\j):"n");}] in {1,...,\NI}{ % loop over nodes in previous layer
			\draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
			\draw[connect] (NI-\j) -- (NO-\i)
			node[pos=0.50] {\contour{white}{$w_{1,\index}$}};
		}
		\else % other light-colored nodes
		\node[node,blue!20!black!80,draw=myblue!20,fill=myblue!5]
		(NO-\i) at (1,\y) {$a_{\index}^{(1)}$};
		\foreach \j in {1,...,\NI}{ % loop over nodes in previous layer
			%\draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
			\draw[connect,myblue!20] (NI-\j) -- (NO-\i);
		}
		\fi
	}
	
	% DOTS
	\path (NI-\NI) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
	\path (NO-\NO) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
	
	% EQUATIONS
	\def\agr#1{{\color{mydarkgreen}a_{#1}^{(0)}}}
	\node[below=17,right=11,mydarkblue,scale=0.95] at (NO-1)
	{$\begin{aligned} %\underset{\text{bias}}{b_1}
			&= \color{mydarkred}\sigma\left( \color{black}
			w_{1,0}\agr{0} + w_{1,1}\agr{1} + \ldots + w_{1,n}\agr{n} + b_1^{(0)}
			\color{mydarkred}\right)\\
			&= \color{mydarkred}\sigma\left( \color{black}
			\sum_{i=1}^{n} w_{1,i}\agr{i} + b_1^{(0)}
			\color{mydarkred}\right)
		\end{aligned}$};
	\node[right,scale=0.9] at (1.3,-1.3)
	{$\begin{aligned}
			{\color{mydarkblue}
				\begin{pmatrix}
					a_{1}^{(1)} \\[0.3em]
					a_{2}^{(1)} \\
					\vdots \\
					a_{m}^{(1)}
			\end{pmatrix}}
			&=
			\color{mydarkred} \beta_j \cdot \sigma\left[ \color{black}
			\begin{pmatrix}
				w_{1,0} & w_{1,1} & \ldots & w_{1,n} \\
				w_{2,0} & w_{2,1} & \ldots & w_{2,n} \\
				\vdots  & \vdots  & \ddots & \vdots  \\
				w_{m,0} & w_{m,1} & \ldots & w_{m,n}
			\end{pmatrix}
			{\color{mydarkgreen}
				\begin{pmatrix}
					x_{1}^{} \\[0.3em]
					x_{2}^{} \\
					\vdots \\
					x_{n}^{}
			\end{pmatrix}}
			+
			\begin{pmatrix}
				\theta_{1}^{} \\[0.3em]
				\theta_{2}^{} \\
				\vdots \\
				\theta_{m}^{}
			\end{pmatrix}
			\color{mydarkred}\right]\\[0.5em]
			{\color{mydarkblue}a^{(1)}}
			&=\beta_j \cdot \color{mydarkred}\sigma\left( \color{black}
			\mathbf{W}^{(0)} {\color{mydarkgreen}a^{(0)}}-\mathbf{\theta_j}^{}
			\color{mydarkred}\right)
			%\color{black},\quad \mathbf{W}^{(0)} \in \mathbb{R}^{m\times n}
		\end{aligned}$};
	
\end{tikzpicture}
			

\begin{definition} (Multilayer feedforward networks) The function that a MFN compute is: 
	$$f(x)=\sum_{j=1}^k \beta_j \cdot \sigma(w_j \cdot x - \theta_j)$$
	where $x \in \mathbb{R}^n$ is the input vector, $k \in \mathbb{N}$ is the number of processing units in the hidden layer, $w_j \in \mathbb{R}^n$ is the weight vector that connects the input to processing unit $j$ in the hidden layer, $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ is an activation function applied element-wise to the vector $w_j^T x - \theta_j$, where $\theta_j \in \mathbb{R}$ is the threshold (or bias) associated with processing unit $j$ in the hidden layer, and $\beta_j \in \mathbb{R}$ is the weight that connects processing unit $j$ in the hidden layer to the output of the network.

\end{definition}
\noindent Let $N_{w}$ be the family of all functions implied by the network's architecture.  If we can show that $N_{w}$ is dense in $C(\mathbb{R}^n)$, we can conclude that for every continuous function $g \in C(\mathbb{R}^n) $ and each compact set $K \subset \mathbb{R}^n$, there is a function $f \in N_{w}$ such that $f$ is a good approximation to $g$ on K. \\ \\
\noindent Under which necessary and sufficient conditions on $\sigma$ will the family of networks $N_w$ be capable of approximating to any desired accuracy any given continuous function?



\end{document}