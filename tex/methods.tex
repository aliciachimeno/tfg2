\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Multilayer Feedforward Networks} \label{ch:methods}
\section{Function Approximation}
\noindent In this paper we take $C(\mathbb{R}^n)$ to be the family of "real world" functions that one may wish to approximate. If we can show that a given set of functions $F$ is dense in $C(\mathbb{R}^n)$, we can conclude that for every continuous function $g \in C(\mathbb{R}^n) $ and each compact set $K \subset \mathbb{R}^n$, there is a function $f \in F$ such that $f$ is a good approximation to $g$ on K. 

\begin{definition}\label{thm:first}
	A \emph{metric} (or \emph{distance}) on a set $X$ is a function $d:X\times X\rightarrow \mathbb{R} $ such that forall $s,t\in X$ the following properties are satisfied:
	\begin{enumerate}
		\item $d(s,t) \geq 0$ and $ \, d(s,t)=0$ if and only if $ s=t$.
		\item $d(s,t)=d(t,s)$.
		\item $d(s,t)\leq d(s,u)+d(u,t)\quad$(\emph{triangular inequality}).
	\end{enumerate}
A \emph{metric space} is a pair $(X,d)$, where $X$ is a set and $d$ is a distance in $X$.
\end{definition}
\noindent  If we take $X$ to be a set of functions, the metric $d(f,g)$ will enable us to measure the distance between functions $f,g \in X$.

\begin{definition}
We denote the support of a function $u$ by $supp(u)= \{x | u(x)\neq 0\}$
\end{definition}

\begin{definition}
	Let $f,g$ be real-valued functions with compact support. We define the \emph{convolution} of $f$ with $g$ as $$(f\ast g)(x)=\int f(x-t)g(t) \, dt$$
\end{definition}

\noindent Lebesgue measure

\begin{definition} A box in $\mathbb{R}^d$ is a set of the form 
	$$ Q = [a_1,b_1]  \times ... \times [a_d,b_d] = \prod_{i=1}^d [a_i,b_i]$$
	The volume of the box is $$ vol(Q)= (b_1,a_1) ... (b_d-a_d)= \prod_{i=1}^d (b_i-a_i)$$
	The \emph{exterior measure} (or outer measure) of a set $E\subseteq \mathbb{R}^d$ is $$ |E|^* = \inf\{\sum_k vol(Q_k)\} $$ 
	where the infimum is taken over all finite or countable collection of boxes $Q_k$ such that $E \subseteq \cup_k Q_k$
\end{definition}

\begin{definition}
A set $E\subseteq \mathbb{R}^n $ is \emph{Lebesgue mesurable} (or mesurable) if $\forall \epsilon >0$, there exist $U$ open set such that $E \subseteq U$ and $|U\setminus E|^* < \epsilon $
\end{definition}

\begin{definition}
	A function $u$ defined almost everywhere on a measurable set $\Omega \in \mathbb{R}^n$ is said to be \emph{essentially bounded} on $\Omega$ if $|u(x)|$ is bounded almost everywhere on $\Omega$. We denote $u\in L^{\infty}(\Omega)$ with the norm $$\|u\|_{L^{\infty}(\Omega)}= inf(\lambda | \{ x : |u(x)| \geq \lambda \} = 0 ) = ess \sup_{x\in \Omega} |u(x)|$$

\end{definition}

\noindent We have that $L^{\infty}(\mathbb{R})$ is the space of essentially bounded functions.
\\ \\ 
Examples and counterexamples of functions essentially bounded. 
\begin{itemize}
\item $f:\Omega \rightarrow $
\end{itemize}

\begin{definition}
	A function u defined almost everywhere on a domain $\Omega$ (a domain is an open set in $\mathbb{R}^n$) is said to be\emph{ locally essentially bounded }on $\Omega$ if for every compact set $K\subset \Omega$, $u\in L^{\infty}(K)$. We denote $u\in L_{loc}^{\infty}(K)$.
\end{definition}

\begin{definition}We say that a set of functions $F\subset L_{loc}^{\infty}(\mathbb{R})$ is \emph{dense} in $C(\mathbb{R}^n)$ if for every function $g\in C(\mathbb{R}^n)$ and for every compact $K\subset \mathbb{R}^n$, there exist a sequence of functions $f_j\in F$ such that $$\lim_{j\rightarrow\infty} \|g-f_j\|_{L^\infty(K)}=0.$$ 
\end{definition}

\section{Results}
\begin{definition} Let $\mathcal{M}$ denote the set of functions which are in $L_{loc}^{\infty}(\mathbb{R})$ and have the following property. The closure of the set of points of discontinuity of any function in $\mathcal{M}$ is of zero Lebesgue measure. 
\begin{propo}
(This implies that) for any $\sigma \in$ $\mathcal{M}$, interval $[a,b] .$ and $\delta >0$, there exists a finite number of open intervals, the union of which we denote by U, of measure $\delta$, such that $\sigma$ is uniformly continuous on $[a,b]/U$. 
\end{propo}
\end{definition}

\begin{definition} 
	$ \mathcal{C}^\infty_0$ functions $\mathcal{C}^\infty$ with compact support.  
\end{definition}

\section{Multilayer Feedforward Network}
\noindent Multilayer feedforward networks are a type of artificial neural network that consist of several layers of interconnected nodes, with each node taking input from the previous layer and producing output for the next layer. The general architecture of a multilayer feedforward network, MFN, consist of: input layer: n-input units,  one/more hidden layers : intermediate processing units, output layer: m output-units. 

\begin{tikzpicture}[x=2.7cm,y=1.6cm]
	\message{^^JNeural network activation}
	\def\NI{4} % number of nodes in input layers
	\def\NO{2} % number of nodes in output layers
	\def\yshift{0.4} % shift last node for dots
	
	% INPUT LAYER
	\foreach \i [evaluate={\c=int(\i==\NI); \y=\NI/2-\i-\c*\yshift; \index=(\i<\NI?int(\i):"n");}]
	in {1,...,\NI}{ % loop over nodes
		\node[node in,outer sep=0.6] (NI-\i) at (0,\y) {$x_{\index}^{}$};
	}
	
	% OUTPUT LAYER
	\foreach \i [evaluate={\c=int(\i==\NO); \y=\NO/2-\i-\c*\yshift; \index=(\i<\NO?int(\i):"m");}]
	in {\NO,...,1}{ % loop over nodes
		\ifnum\i=1 % high-lighted node
		\node[node hidden]
		(NO-\i) at (1,\y) {$y_{\index}^{}$};
		\foreach \j [evaluate={\index=(\j<\NI?int(\j):"n");}] in {1,...,\NI}{ % loop over nodes in previous layer
			\draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
			\draw[connect] (NI-\j) -- (NO-\i)
			node[pos=0.50] {\contour{white}{$w_{1,\index}$}};
		}
		\else % other light-colored nodes
		\node[node,blue!20!black!80,draw=myblue!20,fill=myblue!5]
		(NO-\i) at (1,\y) {$a_{\index}^{(1)}$};
		\foreach \j in {1,...,\NI}{ % loop over nodes in previous layer
			%\draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
			\draw[connect,myblue!20] (NI-\j) -- (NO-\i);
		}
		\fi
	}
	
	% DOTS
	\path (NI-\NI) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
	\path (NO-\NO) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
	
	% EQUATIONS
	\def\agr#1{{\color{mydarkgreen}a_{#1}^{(0)}}}
	\node[below=17,right=11,mydarkblue,scale=0.95] at (NO-1)
	{$\begin{aligned} %\underset{\text{bias}}{b_1}
			&= \color{mydarkred}\sigma\left( \color{black}
			w_{1,0}\agr{0} + w_{1,1}\agr{1} + \ldots + w_{1,n}\agr{n} + b_1^{(0)}
			\color{mydarkred}\right)\\
			&= \color{mydarkred}\sigma\left( \color{black}
			\sum_{i=1}^{n} w_{1,i}\agr{i} + b_1^{(0)}
			\color{mydarkred}\right)
		\end{aligned}$};
	\node[right,scale=0.9] at (1.3,-1.3)
	{$\begin{aligned}
			{\color{mydarkblue}
				\begin{pmatrix}
					a_{1}^{(1)} \\[0.3em]
					a_{2}^{(1)} \\
					\vdots \\
					a_{m}^{(1)}
			\end{pmatrix}}
			&=
			\color{mydarkred} \beta_j \cdot \sigma\left[ \color{black}
			\begin{pmatrix}
				w_{1,0} & w_{1,1} & \ldots & w_{1,n} \\
				w_{2,0} & w_{2,1} & \ldots & w_{2,n} \\
				\vdots  & \vdots  & \ddots & \vdots  \\
				w_{m,0} & w_{m,1} & \ldots & w_{m,n}
			\end{pmatrix}
			{\color{mydarkgreen}
				\begin{pmatrix}
					x_{1}^{} \\[0.3em]
					x_{2}^{} \\
					\vdots \\
					x_{n}^{}
			\end{pmatrix}}
			+
			\begin{pmatrix}
				\theta_{1}^{} \\[0.3em]
				\theta_{2}^{} \\
				\vdots \\
				\theta_{m}^{}
			\end{pmatrix}
			\color{mydarkred}\right]\\[0.5em]
			{\color{mydarkblue}a^{(1)}}
			&=\beta_j \cdot \color{mydarkred}\sigma\left( \color{black}
			\mathbf{W}^{(0)} {\color{mydarkgreen}a^{(0)}}-\mathbf{\theta_j}^{}
			\color{mydarkred}\right)
			%\color{black},\quad \mathbf{W}^{(0)} \in \mathbb{R}^{m\times n}
		\end{aligned}$};
	
\end{tikzpicture}
			

\begin{definition} (Multilayer feedforward networks) The function that a MFN compute is: 
	$$f(x)=\sum_{j=1}^k \beta_j \cdot \sigma(w_j \cdot x - \theta_j)$$
	where $x \in \mathbb{R}^n$ is the input vector, $k \in \mathbb{N}$ is the number of processing units in the hidden layer, $w_j \in \mathbb{R}^n$ is the weight vector that connects the input to processing unit $j$ in the hidden layer, $\sigma : \mathbb{R} \rightarrow \mathbb{R}$ is an activation function applied element-wise to the vector $w_j^T x - \theta_j$, where $\theta_j \in \mathbb{R}$ is the threshold (or bias) associated with processing unit $j$ in the hidden layer, and $\beta_j \in \mathbb{R}$ is the weight that connects processing unit $j$ in the hidden layer to the output of the network.

\end{definition}
\noindent Let $N_{w}$ be the family of all functions implied by the network's architecture.  If we can show that $N_{w}$ is dense in $C(\mathbb{R}^n)$, we can conclude that for every continuous function $g \in C(\mathbb{R}^n) $ and each compact set $K \subset \mathbb{R}^n$, there is a function $f \in N_{w}$ such that $f$ is a good approximation to $g$ on K. \\ \\
\noindent Under which necessary and sufficient conditions on $\sigma$ will the family of networks $N_w$ be capable of approximating to any desired accuracy any given continuous function?


\section{Theorem}
\begin{theorem} Let $ \sigma \in \text{M} $. Set
	$$ \Sigma_n = span\{\sigma(w\cdot x + \theta) : w\in \mathbb{R}^n, \theta \in \mathbb{R} \}$$
	Then $\Sigma_n$ is dense in $\mathcal{C}(\mathbb{R}^n)$ if and only if $\sigma$ is not an algebraic polynomial. 
	
\end{theorem}
\subsection{Why does not contradict the Weierstrass approximation theorem? }


\begin{theorem}(Weierstrass  approximation theorem). 
	Let $f:[a,b]\rightarrow \mathbb{R} $ be a continuous function. Then, there exists polynomials $p_n\in \mathcal{R}[x]$ such that the sequence $(p_n)$ converge uniformly to $f$ on $[a,b]$. 
\end{theorem} 

\begin{corolari}
	The set of polynomial functions $\mathcal{R}^n[x]$ is dense in the space of continuous functions on a compact set $K \subset \mathbb{R}^n$, $\mathcal{C}(K)$. So any continuous function on a compact set can be approximated arbitrarly well by a polynomial. 
\end{corolari}

\noindent The theorem states that: if $\Sigma_n$ is dense in $\mathcal{C}(\mathbb{R}^n)$ then $\sigma$ is not an algebraic polynomial. But why this statment does not contradict the Weierstrass approximation theorem ?
This does not work because $\sigma$ has degree fixed $k$, then any element in the set $\Sigma_n$ has degree at most $k$. Hence, the set $\Sigma_n$ is a finite vector space and can not be dense in $\mathcal{C}(\mathbb{R}^n)$. Not all contiunous functions can be apparoximated with a polynimial of degree fixed, for example: 
(comment per afegir : per exemple una funcio que sigui continua que no es pugui approximar per un polinomi de com a molt grau k , una k tingui grau mes gran que k polinomi de k+1 ?? )

\subsection{Previous results}
 The activation functions that were reported thus far in the literature. 
\begin{theorem} (Hornik Theorem 1). Standard multilayer feedforward networks with a bounded and nonconstant activation function can approximate any function in $L^p(\mu)$ arbitrary well, given a sufficiently large number of hidden units. 
\end{theorem}

\begin{theorem} (Hornik Theorem 2)  Standard multilayer feedforward networks with a continuous, bounded and nonconstant activation function can approximate any continuous function on $X$ arbitrarily well (with respect to the uniform distance) given a sufficiently large number of hidden units. 
\end{theorem}

\noindent
The theorem generalizes Hornik's Theorem 2 by establishing necessary and sufficient conditions for universal approximation. Note that the theorem merely requires "nonpolynomiality" in the activation function. Unlike Hornik's result, the activation functions do not need to be continuous or smooth. This has an important biological interpretation because the activation functions of real neurons may well be discontinuous or even non-elementary.
\subsection{Results}


\begin{definition} The set $L^{p}(\mu)$ contains all mesurable functions $f$ such that: 
	$$ \|f\|_{L^p}(\mu) = \Big( \int_{R^n} |f(x)|^p d\mu(x)\Big)^{1/p} < \infty $$

\end{definition}

\begin{propo}
Let $\mu $ be a non-negative finite measure on $\mathbb{R}$ with compact support, absolutely continous with respect to Lebesgue measure. Then $\Sigma_n$ is dense in $L_p(\mu)$ , $1\leq p < \infty$, if and only of, $\sigma$ is not a polynomial.
\end{propo}

\begin{propo}
If $\sigma\in M$ is not a polynomial (a.e) then, $$ \Sigma_n(\mathcal{A})= span \{ \sigma(\lambda w \cdot x + \theta) : \lambda, \theta \in \mathbb{R}, w \in \mathcal{A} \}$$
is dense in $\mathcal{C}(\mathbb{R}^n)$ for some $\mathcal{A}\subset \mathbb{R}^n$ if and only if there does not exist a nontrivival polynomial vanishing on $\mathcal{A}$.
\end{propo}


\end{document}