%%TC:ignore
\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter*{Abstract}\label{ch:abstract} \addcontentsline{toc}{chapter}{\nameref{ch:abstract}}
\noindent In today's world, many people employ machine learning models, yet only a few understand the underlying mathematics that support them. In this work, our approach to machine learning will incorporate a more analytical perspective, emphasizing function approximation as a core component. How can we find a predictive function from a given dataset and ascertain the existence of such a function? This research seeks to address these concerns by exploring the mathematical foundations of function approximation in machine learning, with a specific focus on function approximation using neural networks.
\\ \\
In particular, we delve into a significant finding, the theorem proposed by \cite{leshno1993multilayer}, which states that a multilayer feedforward network equipped with a non-polynomial activation function can effectively approximate any continuous function. Our work revolves around understanding and reinterpreting the proof, while expanding and providing further details.
Through this study, we aim to bridge the gap between the practical application of machine learning and the mathematical principles that underpin its success.




\chapter*{Resum}

\chapter*{Preface}



blslalblallb en catal√†
\end{document}
%%TC:endignore