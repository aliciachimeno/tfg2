%%TC:ignore
\documentclass[../../main.tex]{subfiles}

\begin{document}



\section*{Abstract} % starred section doesn't get the correct header


\noindent As machine learning models continue to grow in significance, mathematics play a crucial role in comprehending the complex underlying principles behind them. How can we ensure the existence of a predictive function from a given dataset? In this work, we will take an analytical approach to machine learning, emphasizing function approximation as a central component. This research seeks to address these concerns by exploring the mathematical foundations of function approximation in machine learning, with a specific focus on neural networks.
\\ \\
In particular, we delve into a significant finding, the theorem proved by Leshno-Lin-Pinkus-Schocken in 1993 \cite{leshno1993multilayer}, which states that a multilayer feedforward network equipped with a non-polynomial activation function can effectively approximate any continuous function. Our work revolves around understanding and reinterpreting the proof, while expanding and providing further details.
Through this study, we aim to bridge the gap between the practical application of machine learning and the mathematical principles that underpin its success.
\vfill *Cover image generated by an AI (DALL-E).

\newpage

\section*{Resum}
\noindent A mesura que els models de machine learning continuen guanyant rellevància, les matemàtiques juguen un paper essencial en la seva comprensió. Com podem garantir l'existència d'una funció predictiva a partir d'un conjunt de dades donat?  En aquest treball prendrem una visió analitica del machine learning posant èmfasi en l'aproximació de funcions com a component central. Aquesta recerca pretén explorar els fonaments matemàtics de l'aproximació de funcions en l'aprenentatge automàtic amb un focus específic en les xarxes neuronals.
\\ \\
\noindent En particular, aprofundim en una troballa important, el teorema demostrat per Leshno-Lin-Pinkus-Schocken el 1993 \cite{leshno1993multilayer}, que afirma que una xarxa neuronal equipada amb una funció d'activació no polinomial pot aproximar qualsevol funció contínua. El nostre treball gira entorn a comprendre i reinterpretar la demostració, alhora que ampliar i proporcionar més detalls.
A través d'aquest estudi, pretenem establir un nexe entre l'aplicació pràctica de l'aprenentatge automàtic i els principis matemàtics que sustenten el seu èxit.

\end{document}
%%TC:endignore